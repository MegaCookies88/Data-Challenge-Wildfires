---
title: "Wildfire_Challenge"
author: "Mongi Nouira"
date: "3/27/2021"
output: 
  html_document:
    keep_md: true
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center",  cache=TRUE)
```

```{r libraries, include=FALSE}
library("FactoMineR")
library("factoextra")

library("pscl")

library("glmnet")
library("glmnetUtils")

library("doParallel")
library("parallel")

library("randomForest")
library("gbm")

library("mgcv")
```

```{r data, include=FALSE}

# Loading the data
load("~/Documents/Cours/EPFL MA2/R_WorkSpace/Wildfire_Challenge/data_train.RData")

# Loading functions
source("scoring_functions.R")

# Challenge benchmark
S_CNT_bench = 6026
S_BA_bench = 4257

# Wind
data_train_DF$wind <- scale(sqrt(data_train_DF$clim1^2 + data_train_DF$clim2^2))

# Relative humidity
data_train_DF$RH <- 100 * exp(17.625*(data_train_DF$clim3-273.15)/(data_train_DF$clim3-39.11)) /
                          exp(17.625*(data_train_DF$clim4-273.15)/(data_train_DF$clim4-39.11))

# Removing NA in new DF
data_test_DF <- data_train_DF[rowSums(is.na(data_train_DF)) > 0,]
data_train_DF <- data_train_DF[rowSums(is.na(data_train_DF)) == 0,]

# Training : 95/05 - Testing : 07
train <- data_train_DF[data_train_DF$year %in% c(1995,2005,2015),]
test <- data_train_DF[data_train_DF$year %in% c(1997,2007),]

# Scaling : Standard Scale
for (i in 8:length(data_train_DF)) {
  train[,i] = scale(train[,i])
  test[,i] = scale(test[,i])
}
rm(i)

cat("train size =", nrow(train), "- test size =", nrow(test))

```

# Principal Component Analysis (PCA)

```{r PCA, out.width="100%"}

res.pca <- PCA(data_train_DF[,3:38], graph = FALSE)
print(res.pca)

fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 30))

fviz_cos2(res.pca, choice = "var", axes = 1:2)

fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,
             max.overlaps = 3
             )

fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)

```


#-------------------------------------------------------------------------------


# CNT Models

## Benchmark Model : GLM Poisson Log

```{r benchmark}

# Fitting the model
m0 <- glm(CNT ~ . -BA -wind -RH, 
          family = poisson(link="log"), 
          data = train)
summary(m0)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(m0, test)

# Score
S_CNT_0 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_0)

#MLmetrics::MAPE(prediction_cnt[test$CNT>0,], test[test$CNT>0,]$CNT)

```

## GLM Poisson log

```{r glm_poisson}

# Fitting the model
m1 <- glm(CNT ~ -1 + factor(month) 
          + wind + altiMean + clim4 + clim7 + clim8 + clim9 + clim10
          + lc7 + lc11 + lc15 + lc16, 
          family = poisson(link="log"), 
          data = train)
summary(m1)

# Comparing with m0
# anova(m0,m1,test="Chisq")

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(m1, test)

# Score
S_CNT_1 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_1)

```

## Zero Inflated Poisson

```{r zero_poisson}

# Fitting the model
mp0 <- zeroinfl(CNT ~ . -BA -1 -year -clim1 -clim2 -lon -lat -area -altiSD -lc6 -lc17
                | 1, 
                data = train)
summary(mp0)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_zero_poisson(mp0, test)
  
# Score
S_CNT_p0 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_p0)

```

```{r}

# Fitting the model
mp1 <- zeroinfl(CNT ~ . -BA -1 -clim1 -clim2 -year -month -lat -altiSD
                + factor(month) 
                | 1, 
                data = train)
summary(mp1)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_zero_poisson(mp1, test)
  
# Score
S_CNT_p1 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_p1)

```

## GLM m0 Lasso

```{r CNT_lasso}

registerDoParallel(4)

# Lambda grid
grid = 10^seq(-2, 2, length = 100)

# CV
cv.CNT <- cv.glmnet(CNT ~ . -BA -clim1 -clim2, 
                    intercept = TRUE,
                    family = poisson(link="log"),
                    alpha = 1,
                    nfolds = 5,
                    parallel = TRUE,
                    lambda = grid,
                    data = train)
plot(cv.CNT)
bestlam = cv.CNT$lambda.min
cat("Best Lambda =", bestlam)

# Fitting the model
mCNT_lasso <- glmnet(CNT ~ . -BA -clim1 -clim2, 
                     intercept = TRUE,
                     family = poisson(link="log"),
                     alpha = 1,
                     lambda = bestlam,
                     data = train)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(mCNT_lasso, test[,-c(1,2,28,29)])

# Score
S_CNT_lasso <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("\nS_CNT =", S_CNT_lasso)

```


#-------------------------------------------------------------------------------


# BA Models

## Benchmark Model : Mixture of m0 and GLM Gaussian Id on logBA

```{r}

# Fitting the model
BAm0 <- glm(log(BA) ~ . -CNT -wind -RH, 
          family=gaussian(), 
          data=train[train$BA>0,])
summary(BAm0)

# Prediction
p0 <- ppois(0, lambda = predict(m0, test, type="response"), log = FALSE)
prediction_ba_prob <- predict_ba_prob_gaussian_0(p0, BAm0, test)

# Score
S_BA_0 <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_0)

```

## Mixture of Random Forest Classifier and GLM Gaussian Id on logBA

```{r}

# Classifier for BA>0
rf <- randomForest(as.factor(I(BA>0)) ~ . -1 -CNT -clim1 -clim2,
                   data=train)
rf

```

```{r}

# Fitting the model for BA>0
BAmrf <- glm(log(BA) ~ . -CNT -clim1 -clim2, 
            family=gaussian(), 
            data=train[train$BA>0,])
summary(BAmrf)

# Prediction
p0 <- predict(rf, test, type="prob")[,1]
prediction_ba_prob <- predict_ba_prob_gaussian_0(p0, BAmrf, test)

# Score
S_BA_rf <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_rf)

```

## GLM Gaussian Id on log(BA+1)
Tested it on 80k -> better than benchmark with a score of 3932 (about rank 4)

```{r}

# Fitting the model
BAm1 <- glm(log1p(BA) ~ . -1 -CNT -clim1 -clim2 -lc6 -clim6 -lc10 -clim8, 
            family=gaussian(), 
            data=train)
summary(BAm1)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_1(BAm1, test)

# Score
S_BA_1 <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_1)

```

## GAM Gaussian Id on log(BA+1) 
3923 full data

```{r}

# Activate Multi-Threads and start clock
start.time <- Sys.time()
cl <- makeCluster(6)

# Fitting the model
BAm1s <- bam(log1p(BA) ~ -1 + year + month + s(lon,lat,bs="gp",k=100) + s(area,bs="cr",k=12)
             + s(altiMean,bs="cr",k=20) + s(wind,bs="cr",k=12) + s(RH,bs="cr",k=12)
             + s(clim4,bs="cc",k=7)
             + ti(lc11,wind,k=12) + ti(lc12,wind,k=12) + ti(lc1,wind,k=12) 
             + ti(lc7,wind,k=12) + ti(lc14,wind,k=12) + s(lc18,bs="cr",k=30),
             data=train,
             family=gaussian(),
             discrete=FALSE,
             cluster=cl)

summary(BAm1s)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_1(BAm1s, test)

# Close Clusters and stop clock
if (!is.null(cl)) stopCluster(cl)
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

# Score
S_BA_1s <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_1s)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

```

```{r}
plot(BAm1s)
```

```{r}
gam.check(BAm1s)
```

## Speeding previous RF model

```{r}

hist(log(train[train$BA>=1,]$BA), breaks=50)
par(new=TRUE)
abline(v=log(u_ba[-1]), col="red")

```

```{r}

# Classifier for BA>=1
rf <- randomForest(as.factor(I(BA>=1)) ~ . -CNT -clim1 -clim2,
                   data=train)
rf

```

```{r}

# Activate Multi-Threads and start clock
start.time <- Sys.time()
cl <- makeCluster(6)

# Fitting the model for BA>0
BAmrfs <- bam(log(BA) ~ -1 + year + month + s(lon,lat,bs="gp",k=100) + s(area,bs="cr",k=12)
              + s(altiMean,bs="cr",k=20) + s(wind,bs="cr",k=12) + s(RH,bs="cr",k=12)
              + s(clim4,bs="cc",k=7)
              + ti(lc11,wind,k=12) + ti(lc12,wind,k=12) + ti(lc1,wind,k=12) 
              + ti(lc7,wind,k=12) + ti(lc14,wind,k=12) + s(lc18,bs="cr",k=12),
              data=train[train$BA>=1,],
              family=gaussian(),
              discrete=FALSE,
              cluster=cl)

summary(BAmrfs)

# Prediction
p0 <- predict(rf, test, type="prob")[,1]
prediction_ba_prob <- predict_ba_prob_gaussian_0(p0, BAmrfs, test)

# Close Clusters and stop clock
if (!is.null(cl)) stopCluster(cl)
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

# Score
S_BA_rfs <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_rfs)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

```

## Generalized Boosted Regression on log(BA+1)

```{r}

registerDoParallel(10)

# tic
start.time <- Sys.time()

set.seed(42)
BAmGBM <- gbm(log1p(BA) ~ . -CNT -clim1 -clim2, 
              distribution = "gaussian",
              interaction.depth = 5,
              n.trees = 500,
              n.minobsinnode = 10,
              shrinkage = 0.1,
              cv.folds = 10,
              n.cores = 10,
              data = train)

# tac
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_gbm1(BAmGBM, test)

# Score
S_BA_GBM <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_GBM)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

stopImplicitCluster()

```

## GBM on log(BA+1) improvement :

```{r}

# Adding mean fire per voxels
mean_CNT_voxel <- aggregate(train[,1], by=list(train$lon,train$lat), mean)
train$mean_fire <- mapply(function(x,y) mean_CNT_voxel[mean_CNT_voxel$Group.1==x & mean_CNT_voxel$Group.2==y,3],
                          train$lon, train$lat)
test$mean_fire <- mapply(function(x,y) mean_CNT_voxel[mean_CNT_voxel$Group.1==x & mean_CNT_voxel$Group.2==y,3],
                          test$lon, test$lat)

registerDoParallel(10)

# tic
start.time <- Sys.time()

set.seed(42)

BAmGBM_1 <- gbm(log1p(BA) ~ . -CNT -clim1 -clim2, 
                distribution = "gaussian",
                interaction.depth = 5,
                n.trees = 500,
                n.minobsinnode = 10,
                shrinkage = 0.1,
                cv.folds = 10,
                n.cores = 10,
                data = train)

BAmGBM_2 <- gbm(log1p(BA) ~ . -clim1 -clim2, 
                distribution = "gaussian",
                interaction.depth = 5,
                n.trees = 500,
                n.minobsinnode = 10,
                shrinkage = 0.1,
                cv.folds = 10,
                n.cores = 10,
                data = train)

# tac
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_gbm1(BAmGBM_1, test)
prediction_ba_prob_2 <- predict_ba_prob_gaussian_gbm1(BAmGBM_2, test)

# Random combination based on weights : CNT 64% NA and 36 not NA
test_ind <- sample(seq_len(nrow(test)), size = floor(0.64 * nrow(test)))
prediction_ba_prob[-test_ind,] <- prediction_ba_prob_2[-test_ind,]

# Score
S_BA_GBM_F <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_GBM_F)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

stopImplicitCluster()
  
```




