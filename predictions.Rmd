---
title: "Wildfire_Challenge"
author: "Mongi Nouira"
date: "3/27/2021"
output: html_document
---

```{r libraries, include=FALSE}
library("FactoMineR")
library("factoextra")

library("pscl")

library("glmnet")
library("glmnetUtils")

library("doParallel")
library("parallel")

library("randomForest")
library("gbm")

library("mgcv")
```

```{r data, echo=FALSE}

# Loading the data
load("~/Documents/Cours/EPFL MA2/R_WorkSpace/Wildfire_Challenge/data_train.RData")

# Loading functions
source("scoring_functions.R")

# Challenge benchmark
S_CNT_bench = 6026
S_BA_bench = 4257

# Wind
data_train_DF$wind <- scale(sqrt(data_train_DF$clim1^2 + data_train_DF$clim2^2))

# Relative humidity
data_train_DF$RH <- 100 * exp(17.625*(data_train_DF$clim3-273.15)/(data_train_DF$clim3-39.11)) /
                          exp(17.625*(data_train_DF$clim4-273.15)/(data_train_DF$clim4-39.11))

# Scaling : Standard Scale
for (i in 8:length(data_train_DF)) {
  data_train_DF[,i] = scale(data_train_DF[,i])
}

# Removing NA in new DF
data_test_DF <- data_train_DF[rowSums(is.na(data_train_DF)) > 0,]
data_train_DF <- data_train_DF[rowSums(is.na(data_train_DF)) == 0,]

# 80% training / 20% testing 
# First I tried about 80k missing CNT as in the challenge
# Then I chose to let the benchmark score be a little more than the one in the challenge
set.seed(42)
train_ind <- sample(seq_len(nrow(data_train_DF)), size = floor(0.822 * nrow(data_train_DF)))
train <- data_train_DF[train_ind, ]
test <- data_train_DF[-train_ind, ]

# Training : 95/05 - Testing : 07
train <- data_train_DF[data_train_DF$year %in% c(1995,2005,2015),]
test <- data_train_DF[data_train_DF$year %in% c(1997,2007),]

cat("train size =", nrow(train), "- test size =", nrow(test))

```

# Principal Component Analysis (PCA)

```{r PCA, echo=FALSE, out.width="100%"}

res.pca <- PCA(data_train_DF[,3:38], graph = FALSE)
print(res.pca)

fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 30))

fviz_cos2(res.pca, choice = "var", axes = 1:2)

fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,
             max.overlaps = 3
             )

fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)

```

# CNT Models

## Benchmark Model : GLM Poisson Log

```{r benchmark, echo=FALSE}

# Fitting the model
m0 <- glm(CNT ~ . -BA -wind -RH, 
          family = poisson(link="log"), 
          data = train)
summary(m0)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(m0, test)

# Score
S_CNT_0 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_0)

#MLmetrics::MAPE(prediction_cnt[test$CNT>0,], test[test$CNT>0,]$CNT)

```

## GLM Poisson log

```{r glm_poisson, echo=FALSE}

# Fitting the model
m1 <- glm(CNT ~ -1 + factor(month) 
          + wind + altiMean + clim4 + clim7 + clim8 + clim9 + clim10
          + lc7 + lc11 + lc15 + lc16, 
          family = poisson(link="log"), 
          data = train)
summary(m1)

# Comparing with m0
# anova(m0,m1,test="Chisq")

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(m1, test)

# Score
S_CNT_1 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_1)

```

## Zero Inflated Poisson

```{r zero_poisson, echo=FALSE}

# Fitting the model
mp0 <- zeroinfl(CNT ~ . -BA -1 -year -clim1 -clim2 -lon -lat -area -altiSD -lc6 -lc17
                | 1, 
                data = train)
summary(mp0)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_zero_poisson(mp0, test)
  
# Score
S_CNT_p0 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_p0)

```

```{r echo=FALSE}

# Fitting the model
mp1 <- zeroinfl(CNT ~ . -BA -1 -clim1 -clim2 -year -month -lat -altiSD
                + factor(month) 
                | 1, 
                data = train)
summary(mp1)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_zero_poisson(mp1, test)
  
# Score
S_CNT_p1 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_p1)

```

## GLM m0 Lasso

```{r CNT_lasso, echo=FALSE}

registerDoParallel(4)

# Lambda grid
grid = 10^seq(-2, 2, length = 100)

# CV
cv.CNT <- cv.glmnet(CNT ~ . -BA -clim1 -clim2, 
                    intercept = TRUE,
                    family = poisson(link="log"),
                    alpha = 1,
                    nfolds = 5,
                    parallel = TRUE,
                    lambda = grid,
                    data = train)
plot(cv.CNT)
bestlam = cv.CNT$lambda.min
cat("Best Lambda =", bestlam)

# Fitting the model
mCNT_lasso <- glmnet(CNT ~ . -BA -clim1 -clim2, 
                     intercept = TRUE,
                     family = poisson(link="log"),
                     alpha = 1,
                     lambda = bestlam,
                     data = train)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(mCNT_lasso, test[,-c(1,2,28,29)])

# Score
S_CNT_lasso <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("\nS_CNT =", S_CNT_lasso)

```


#-------------------------------------------------------------------------------


# BA Models

## Benchmark Model : Mixture of m0 and GLM Gaussian Id on logBA

```{r echo=FALSE}

# Fitting the model
BAm0 <- glm(log(BA) ~ . -CNT -wind -RH, 
          family=gaussian(), 
          data=train[train$BA>0,])
summary(BAm0)

# Prediction
p0 <- ppois(0, lambda = predict(m0, test, type="response"), log = FALSE)
prediction_ba_prob <- predict_ba_prob_gaussian_0(p0, BAm0, test)

# Score
S_BA_0 <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_0)

```

## Mixture of Random Forest Classifier and GLM Gaussian Id on logBA

```{r echo=FALSE}

# Classifier for BA>0
rf <- randomForest(as.factor(I(BA>0)) ~ . -1 -CNT -clim1 -clim2,
                   data=train)
rf

```

```{r echo=FALSE}

# Fitting the model for BA>0
BAmrf <- glm(log(BA) ~ . -CNT -clim1 -clim2, 
            family=gaussian(), 
            data=train[train$BA>0,])
summary(BAmrf)

# Prediction
p0 <- predict(rf, test, type="prob")[,1]
prediction_ba_prob <- predict_ba_prob_gaussian_0(p0, BAmrf, test)

# Score
S_BA_rf <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_rf)

```

## GLM Gaussian Id on log(BA+1)
Tested it on 80k -> better than benchmark with a score of 3932 (about rank 4)

```{r echo=FALSE}

# Fitting the model
BAm1 <- glm(log1p(BA) ~ . -1 -CNT -clim1 -clim2 -lc6 -clim6 -lc10 -clim8, 
            family=gaussian(), 
            data=train)
summary(BAm1)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_1(BAm1, test)

# Score
S_BA_1 <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_1)

```

## GAM Gaussian Id on log(BA+1) 
3923 full data

```{r echo=FALSE}

# Activate Multi-Threads and start clock
start.time <- Sys.time()
cl <- makeCluster(6)

# Fitting the model
BAm1s <- bam(log1p(BA) ~ -1 + year + month + s(lon,lat,bs="gp",k=100) + s(area,bs="cr",k=12)
             + s(altiMean,bs="cr",k=20) + s(wind,bs="cr",k=12) + s(RH,bs="cr",k=12)
             + s(clim4,bs="cc",k=7)
             + ti(lc11,wind,k=12) + ti(lc12,wind,k=12) + ti(lc1,wind,k=12) 
             + ti(lc7,wind,k=12) + ti(lc14,wind,k=12) + s(lc18,bs="cr",k=30),
             data=train,
             family=gaussian(),
             discrete=FALSE,
             cluster=cl)

summary(BAm1s)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_1(BAm1s, test)

# Close Clusters and stop clock
if (!is.null(cl)) stopCluster(cl)
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

# Score
S_BA_1s <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_1s)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

```

```{r echo=FALSE}
plot(BAm1s)
```

```{r echo=FALSE}
gam.check(BAm1s)
```

Questions :
1. The score is nearly the same as the glm without smoothing : Does it mean that the score is low because they both fail to model the tails ?
2. Speaking of tails, the QQ plot for bam clearly shows there is a problem with tails : Since I used the gaussian family, does it mean if I use the " scat : scaled t for heavy tailed data that would otherwise be modelled as Gaussian " it should improve the model? Since the camputation time is very different (2min vs more than 1h30 (stopped)), can we have an idea before testing?
3. I also wanted to try the Gumbel location-scale distribution but it is only for gam and not bam so it should be slow.
4. gam.check seems to be useful for basis dimension selection but how do we use it? Even in the gam book and mgcv docu it isn't clear for me.

## Speeding previous RF model

```{r echo=FALSE}

hist(log(train[train$BA>=1,]$BA), breaks=50)
par(new=TRUE)
abline(v=log(u_ba[-1]), col="red")

```

```{r echol=FALSE}

# Classifier for BA>=1
rf <- randomForest(as.factor(I(BA>=1)) ~ . -CNT -clim1 -clim2,
                   data=train)
rf

```

```{r echol=FALSE}

# Activate Multi-Threads and start clock
start.time <- Sys.time()
cl <- makeCluster(6)

# Fitting the model for BA>0
BAmrfs <- bam(log(BA) ~ -1 + year + month + s(lon,lat,bs="gp",k=100) + s(area,bs="cr",k=12)
              + s(altiMean,bs="cr",k=20) + s(wind,bs="cr",k=12) + s(RH,bs="cr",k=12)
              + s(clim4,bs="cc",k=7)
              + ti(lc11,wind,k=12) + ti(lc12,wind,k=12) + ti(lc1,wind,k=12) 
              + ti(lc7,wind,k=12) + ti(lc14,wind,k=12) + s(lc18,bs="cr",k=12),
              data=train[train$BA>=1,],
              family=gaussian(),
              discrete=FALSE,
              cluster=cl)

summary(BAmrfs)

# Prediction
p0 <- predict(rf, test, type="prob")[,1]
prediction_ba_prob <- predict_ba_prob_gaussian_0(p0, BAmrfs, test)

# Close Clusters and stop clock
if (!is.null(cl)) stopCluster(cl)
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

# Score
S_BA_rfs <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_rfs)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

```

## Generalized Boosted Regression on log(BA+1)

```{r echo=FALSE}

registerDoParallel(6)

# tic
start.time <- Sys.time()

set.seed(42)
BAmGBM <- gbm(log1p(BA) ~ . -CNT -clim1 -clim2, 
              distribution = "gaussian",
              interaction.depth = 5,
              n.trees = 500,
              n.minobsinnode = 10,
              shrinkage = 0.1,
              cv.folds = 10,
              n.cores = 6,
              data = train)

# tac
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_gbm1(BAmGBM, test)

# Score
S_BA_GBM <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_GBM)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

stopImplicitCluster()

```

## Regression-Enhanced Random Forest Algorithm on log(BA+1)

```{r echo=FALSE}

registerDoParallel(6)

# Lambda grid
grid = 10^seq(-3, 3, length = 100)

# CV
cv.BA <- cv.glmnet(log1p(BA) ~ . -CNT -clim1 -clim2, 
                   family = gaussian(),
                   alpha = 1,
                   nfolds = 5,
                   parallel = TRUE,
                   lambda = grid,
                   data = train)
plot(cv.BA)

bestlam = cv.BA$lambda.min
cat("Best Lambda =", bestlam)

stopImplicitCluster()

```

```{r echo=FALSE}

# New training set based on residuals
new.train <- train
new.train["resid"] <- train$BA - predict(cv.BA, train, s="lambda.min")

# tic
start.time <- Sys.time()

# Random Forest
rerf <- randomForest(log1p(BA) ~ . -CNT -clim1 -clim2,
                     data=new.train)

# tac
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

rerf

```

```{r echo=FALSE}

# Prediction 
prediction_ba_prob <- predict_ba_prob_gaussian_rerf(cv.BA, rerf, test)

# Score
S_BA_rerf <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_rerf)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

```

```{r echo=FALSE}

registerDoParallel(6)

# tic
start.time <- Sys.time()

set.seed(42)
reGBM <-gbm(log1p(BA) ~ . -CNT -clim1 -clim2, 
            distribution = "gaussian",
            interaction.depth = 5,
            n.trees = 500,
            n.minobsinnode = 10,
            shrinkage = 0.1,
            cv.folds = 10,
            n.cores = 6,
            data = new.train)

# tac
end.time <- Sys.time()
elapsed.time <- round((end.time - start.time), 3)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_rerf(cv.BA, reGBM, test)

# Score
S_BA_reGBM <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_reGBM)

# Time to fit the model
cat("\n\nTime to fit the model :", elapsed.time)

stopImplicitCluster()

```


