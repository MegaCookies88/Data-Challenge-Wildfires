---
title: "Wildfire_Challenge"
author: "Mongi Nouira"
date: "3/27/2021"
output: html_document
---

```{r data, echo=FALSE}

# Loading the data
load("~/Documents/Cours/EPFL MA2/R_WorkSpace/Wildfire_Challenge/data_train.RData")

# Loading functions
source("scoring_functions.R")

# Challenge benchmark
S_CNT_bench = 0.075
S_BA_bench = 0.053

# Wind
data_train_DF$wind <- scale(sqrt(data_train_DF$clim1^2 + data_train_DF$clim2^2))

# Relative humidity
data_train_DF$RH <- 100 * exp(17.625*(data_train_DF$clim3-273.15)/(data_train_DF$clim3-39.11)) /
                          exp(17.625*(data_train_DF$clim4-273.15)/(data_train_DF$clim4-39.11))

# Scaling : Standard Scale
for (i in 8:length(data_train_DF)) {
  data_train_DF[,i] = scale(data_train_DF[,i])
}

# Removing NA in new DF
data_test_DF <- data_train_DF[rowSums(is.na(data_train_DF)) > 0,]
data_train_DF <- data_train_DF[rowSums(is.na(data_train_DF)) == 0,]

# 80% training / 20% testing 
# First I tried about 80k missing CNT as in the challenge
# Then I chose to let the benchmark score be a little more than the one in the challenge
set.seed(42)
train_ind <- sample(seq_len(nrow(data_train_DF)), size = floor(0.822 * nrow(data_train_DF)))
train <- data_train_DF[train_ind, ]
test <- data_train_DF[-train_ind, ]

# Training : 95/05/15 - Testing : 07/08
train <- data_train_DF[data_train_DF$year %in% c(1995,2005),]
test <- data_train_DF[data_train_DF$year %in% c(2007),]

cat("train size =", nrow(train), "- test size =", nrow(test))

```

# Principal Component Analysis (PCA)

```{r PCA, echo=FALSE, out.width="100%"}

library("FactoMineR")
library("factoextra")

res.pca <- PCA(data_train_DF[,3:38], graph = FALSE)
print(res.pca)

fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 30))

fviz_cos2(res.pca, choice = "var", axes = 1:2)

fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,
             max.overlaps = 3
             )

fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)

```

# CNT Models

## Benchmark Model : GLM Poisson Log

```{r benchmark, echo=FALSE}

# Fitting the model
m0 <- glm(CNT ~ . -BA -wind -RH, 
          family = poisson(link="log"), 
          data = train)
summary(m0)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(m0, test)

# Score
S_CNT_0 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_0)

#MLmetrics::MAPE(prediction_cnt[test$CNT>0,], test[test$CNT>0,]$CNT)

```

## GLM Poisson log

```{r glm_poisson, echo=FALSE}

# Fitting the model
m1 <- glm(CNT ~ -1 + factor(month) 
          + wind + altiMean + clim4 + clim7 + clim8 + clim9 + clim10
          + lc7 + lc11 + lc15 + lc16, 
          family = poisson(link="log"), 
          data = train)
summary(m1)

# Comparing with m0
# anova(m0,m1,test="Chisq")

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(m1, test)

# Score
S_CNT_1 <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_1)

#MLmetrics::MAPE(prediction_cnt[test$CNT>0,], test[test$CNT>0,]$CNT)

```

## Zero Inflated Poisson

```{r zero_poisson, echo=FALSE}

library("pscl")

# Fitting the model
m0p <- zeroinfl(CNT ~ . -BA -1 -year -clim1 -clim2 -lon -lat -area -altiSD -lc6 -lc17
                | 1, 
                data = train)
summary(m0p)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_zero_poisson(m0p, test)
  
# Score
S_CNT_0p <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_0p)

```
```{r echo=FALSE}

# Fitting the model
m1p <- zeroinfl(CNT ~ . -BA -1 -clim1 -clim2 -year -month -lat -altiSD
                + factor(month) 
                | 1, 
                data = train)
summary(m1p)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_zero_poisson(m1p, test)
  
# Score
S_CNT_1p <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_1p)

```

## GLM m0 Lasso

```{r CNT_lasso, echo=FALSE}

library("glmnet")
library("glmnetUtils")

# Lambda grid
grid = 10^seq(-2, 2, length = 5)

# CV
cv.out <- cv.glmnet(CNT ~ . -BA -clim1 -clim2, 
                    family = poisson(link="log"),
                    alpha = 1,
                    nfolds = 5,
                    data = train)
plot(cv.out)
bestlam = cv.out$lambda.min

# Fitting the model
mCNT_lasso <- glmnet(CNT ~ . -BA -clim1 -clim2, 
          family = poisson(link="log"),
          alpha = 1,
          lambda = bestlam,
          data = train)

# Prediction
prediction_cnt_prob <- predict_cnt_prob_poisson(mCNT_lasso, test[,-c(1,2,28,29)])

# Score
S_CNT_lasso <- get_score_cnt(prediction_cnt_prob, test$CNT, u_cnt, weights_cnt)
cat("S_CNT =", S_CNT_lasso)

```



# BA Models

## Benchmark Model : Mixture of m0 and GLM Gaussian Id on logBA

```{r echo=FALSE}

# Fitting the model
BAm0 <- glm(log(BA) ~ . -CNT -wind -RH, 
          family=gaussian(), 
          data=train[train$BA>0,])
summary(BAm0)

# Prediction
p0 <- ppois(0, lambda = predict(m0, test, type="response"), log = FALSE)
prediction_ba_prob <- predict_ba_prob_gaussian_0(p0, BAm0, test)

# Score
S_BA_0 <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_0)

```

## Mixture of Random Forest Classifier and GLM Gaussian Id on logBA

```{r echo=FALSE}

library("randomForest")

# Classifier for BA>0
rf <- randomForest(as.factor(I(BA>0)) ~ . -1 -CNT,
                   data=train)
rf

```

```{r echo=FALSE}

# Fitting the model for BA>0
BAmrf <- glm(log(BA) ~ . -CNT -clim1 -clim2, 
            family=gaussian(), 
            data=train[train$BA>0,])
summary(BAmrf)

# Prediction
p0 <- predict(rf, test, type="prob")[,1]
prediction_ba_prob <- predict_ba_prob_gaussian_0(p0, BAmrf, test)

# Score
S_BA_rf <- get_score_ba(prediction_ba_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_rf)

```

## GLM Gaussian Id on log(BA+1)
Tested it on 80k -> better than benchmark with a score of 3963 (about rank 4)

```{r echo=FALSE}

# Fitting the model
BAm1 <- glm(log1p(BA) ~ . -1 -CNT -clim1 -clim2 -lc6 -clim6 -lc10 -clim8, 
            family=gaussian(), 
            data=data_train_DF[train_ind, ])
summary(BAm1)

# Prediction
prediction_ba_prob <- predict_ba_prob_gaussian_1(BAm1, data_train_DF[-train_ind, ])

# Score
S_BA_1 <- get_score_ba(prediction_ba_prob, data_train_DF[-train_ind, ]$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_1)

```

```{r BA_lasso, eval=FALSE, include=FALSE}

# Lambda grid
grid = 10^seq(-2, 2, length = 5)

# CV
cv.out <- cv.glmnet(log1p(BA) ~ . -1 -CNT -clim1 -clim2, 
                    family = gaussian(),
                    alpha = 1,
                    nfolds = 5,
                    data = train)
plot(cv.out)
bestlam = cv.out$lambda.min

# Fitting the model
mBA_lasso <- glmnet(log1p(BA) ~ . -1 -CNT -clim1 -clim2, 
          family = gaussian(),
          alpha = 1,
          lambda = bestlam,
          data = train)

# Prediction (not possible in glmnet to get se.fit so no distribution function)
prediction_cnt_prob <- predict_ba_prob_gaussian_1(mBA_lasso, test[,-c(1,2,28,29)])

# Score
S_BA_lasso <- get_score_cnt(prediction_bat_prob, test$BA, u_ba, weights_ba)
cat("S_BA =", S_BA_lasso)

```
